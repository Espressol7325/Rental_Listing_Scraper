{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login with account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.by import By\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(\"https://www.facebook.com/\")\n",
    "\n",
    "sleep(5)\n",
    "txtUser = driver.find_element(By.ID, \"email\")\n",
    "txtUser.send_keys(os.getenv(\"user_email\"))\n",
    "\n",
    "txtPass = driver.find_element(By.ID, \"pass\")\n",
    "txtPass.send_keys(os.getenv(\"user_password\"))\n",
    "\n",
    "login = driver.find_element(By.NAME, \"login\")\n",
    "login.click()\n",
    "\n",
    "sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get login cookie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "cookies = driver.get_cookies()\n",
    "\n",
    "with open(\"facebook_cookies.json\", \"w\") as file:\n",
    "    json.dump(cookies, file)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login with cookie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đăng nhập thành công!\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import json\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "with open(\"facebook_cookies.json\", \"r\") as file:\n",
    "    cookies = json.load(file)\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "\n",
    "driver.get(\"https://www.facebook.com/groups/281184089051767\")\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "for cookie in cookies:\n",
    "    driver.add_cookie(cookie)\n",
    "\n",
    "time.sleep(2)\n",
    "driver.refresh()\n",
    "time.sleep(5)\n",
    "\n",
    "try:\n",
    "    driver.find_element(By.CSS_SELECTOR, \"div[aria-label='Search']\")\n",
    "    print(\"Đăng nhập thành công!\")\n",
    "except:\n",
    "    print(\"Đăng nhập thất bại!\")\n",
    "\n",
    "# time.sleep(3)\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove old log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully removed facebook_scraper.log log\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "log_file = 'facebook_scraper.log'\n",
    "if os.path.exists(log_file):\n",
    "    # Close all handlers to release the file\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        handler.close()\n",
    "        logging.root.removeHandler(handler)\n",
    "\n",
    "    # Now try to remove the file\n",
    "    try:\n",
    "        os.remove(log_file)\n",
    "        print(f\"Successfully removed {log_file} log\")\n",
    "    except PermissionError:\n",
    "        print(\n",
    "            f\"Could not remove {log_file} - file is still in use. Will create a new log file.\")\n",
    "\n",
    "# Reconfigure logging after removing handlers\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filemode='w',\n",
    "    filename='facebook_scraper.log'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FB CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import hashlib\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class FacebookScraperLogger:\n",
    "    @staticmethod\n",
    "    def setup():\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "            handlers=[\n",
    "                logging.FileHandler(\"facebook_scraper.log\", encoding='utf-8'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        return logging.getLogger(\"FacebookGroupScraper\")\n",
    "\n",
    "class BrowserManager:\n",
    "    @staticmethod\n",
    "    def get_random_user_agent():\n",
    "        user_agents = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0 Safari/537.36\"\n",
    "        ]\n",
    "        return random.choice(user_agents)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_browser(headless=False):\n",
    "        options = Options()\n",
    "        if headless:\n",
    "            options.add_argument(\"--headless\")\n",
    "        options.add_argument(\"--disable-notifications\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(f\"user-agent={BrowserManager.get_random_user_agent()}\")\n",
    "        return webdriver.Chrome(options=options)\n",
    "\n",
    "class FacebookGroupScraper:\n",
    "    def __init__(self, headless, cookies_file, config_file):\n",
    "        self.logger = FacebookScraperLogger.setup()\n",
    "        self.driver = BrowserManager.create_browser(headless)\n",
    "        self.cookies_file = cookies_file\n",
    "        self.districts = []\n",
    "        self.wards = {}\n",
    "        self.streets = {}\n",
    "        self.amenity_patterns = {}\n",
    "        self.load_location_config(config_file)\n",
    "        self.logger.info(\"Facebook group scraper initialized\")\n",
    "\n",
    "    def load_location_config(self, config_file):\n",
    "        try:\n",
    "            with open(config_file, 'r', encoding='utf-8') as f:\n",
    "                config = json.load(f)\n",
    "            self.districts = config.get('districts', [])\n",
    "            self.wards = config.get('wards', {})\n",
    "            self.amenity_patterns = config.get('amenity_patterns', {})\n",
    "            self.logger.info(f\"Loaded {len(self.districts)} districts, {len(self.wards)} ward mappings\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading config file: {e}\")\n",
    "            self.districts, self.wards, self.amenity_patterns = [], {}, {}\n",
    "\n",
    "    def generate_content_hash(self, content):\n",
    "        if not content:\n",
    "            return \"\"\n",
    "        normalized_content = ' '.join(content.lower().split())\n",
    "        return hashlib.md5(normalized_content.encode('utf-8')).hexdigest()\n",
    "\n",
    "    def load_cookies(self):\n",
    "        try:\n",
    "            with open(self.cookies_file, \"r\") as file:\n",
    "                cookies = json.load(file)\n",
    "            for cookie in cookies:\n",
    "                self.driver.add_cookie(cookie)\n",
    "            self.logger.info(f\"Cookies loaded from {self.cookies_file}\")\n",
    "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "            self.logger.error(f\"Cookie file error: {e}\")\n",
    "\n",
    "    def login(self):\n",
    "        self.logger.info(\"Logging into Facebook...\")\n",
    "        self.driver.get(\"https://www.facebook.com/\")\n",
    "        WebDriverWait(self.driver, 5).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        if self.cookies_file:\n",
    "            self.load_cookies()\n",
    "            self.driver.refresh()\n",
    "        return self.verify_login_status()\n",
    "\n",
    "    def verify_login_status(self):\n",
    "        try:\n",
    "            WebDriverWait(self.driver, 5).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"input[placeholder='Search Facebook']\"))\n",
    "            )\n",
    "            self.logger.info(\"Login successful\")\n",
    "            return True\n",
    "        except (TimeoutException, NoSuchElementException):\n",
    "            self.logger.error(\"Login failed - search bar not found\")\n",
    "            return False\n",
    "\n",
    "    def expand_post_content(self, post_element):\n",
    "        try:\n",
    "            see_more_buttons = post_element.find_elements(\n",
    "                By.XPATH, \".//div[@role='button' and contains(text(), 'See more') or contains(text(), 'Xem thêm')]\")\n",
    "            for btn in see_more_buttons:\n",
    "                self.driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                time.sleep(0.5)\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Failed to expand post: {e}\")\n",
    "\n",
    "    def extract_post_date(self, post_element):\n",
    "        try:\n",
    "            span_elem = post_element.find_element(By.CSS_SELECTOR,\n",
    "                \"span.html-span.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x1hl2dhg.x16tdsg8.x1vvkbs.x4k7w5x.x1h91t0o.x1h9r5lt.x1jfb8zj.xv2umb2.x1beo9mf.xaigb6o.x12ejxvf.x3igimt.xarpa2k.xedcshv.x1lytzrv.x1t2pt76.x7ja8zs.x1qrby5j\"\n",
    "            )\n",
    "            self.driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", span_elem)\n",
    "            ActionChains(self.driver).move_to_element(span_elem).perform()\n",
    "            WebDriverWait(self.driver, 5).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"div.x11i5rnm.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x78zum5.xjpr12u.xr9ek0c.x3ieub6.x6s0dn4\")))\n",
    "            date_tooltip = self.driver.find_element(By.CSS_SELECTOR, \"div.x11i5rnm.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x78zum5.xjpr12u.xr9ek0c.x3ieub6.x6s0dn4\")\n",
    "            return self.format_date(date_tooltip.text.strip())\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Failed to extract date: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def format_date(self, date_string):\n",
    "        try:\n",
    "            if 'at' not in date_string:\n",
    "                return date_string\n",
    "            date_part, time_part = date_string.split(' at ')\n",
    "            date_words = date_part.split()\n",
    "            day = date_words[-3].zfill(2)\n",
    "            month_map = {\n",
    "                'January': '01', 'February': '02', 'March': '03', 'April': '04',\n",
    "                'May': '05', 'June': '06', 'July': '07', 'August': '08',\n",
    "                'September': '09', 'October': '10', 'November': '11', 'December': '12'\n",
    "            }\n",
    "            month = month_map.get(date_words[-2], '01')\n",
    "            year = date_words[-1]\n",
    "            return f\"{year}-{month}-{day} {time_part}:00\"\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Failed to format date '{date_string}': {e}\")\n",
    "            return date_string\n",
    "\n",
    "    def extract_post_content(self, post_element):\n",
    "        \"\"\"Extract post content using multiple fallback methods.\"\"\"\n",
    "        selectors = [\n",
    "            \".//div[@data-ad-rendering-role='story_message']\",\n",
    "            \".//div[contains(@class, 'x6s0dn4') and contains(@class, 'xh8yej3')]\",\n",
    "            \".//div[@class='xdj266r x11i5rnm xat24cr x1mh8g0r x1vvkbs x126k92a']\"\n",
    "        ]\n",
    "\n",
    "        for selector in selectors:\n",
    "            try:\n",
    "                content_element = post_element.find_element(By.XPATH, selector)\n",
    "                return content_element.text\n",
    "            except NoSuchElementException:\n",
    "                continue\n",
    "\n",
    "        self.logger.error(\"No content extracted with any selector\")\n",
    "        return \"\"\n",
    "\n",
    "    def _parse_price(self, content: str) -> int:\n",
    "        try:\n",
    "            content_lower = content.lower()\n",
    "            \n",
    "            match = re.search(r'(\\d+[.,]?\\d*)\\s*(triệu|tr|tỷ|ty|trieu)\\b', content_lower)\n",
    "            if match:\n",
    "                value = float(match.group(1).replace(',', '.'))\n",
    "                unit = match.group(2)\n",
    "                return int(value * (1_000_000_000 if unit in ['tỷ', 'ty'] else 1_000_000))\n",
    "            \n",
    "            match = re.search(r'(\\d+)tr(\\d)\\b', content_lower)\n",
    "            if match:\n",
    "                value = float(f\"{match.group(1)}.{match.group(2)}\")\n",
    "                return int(value * 1_000_000)\n",
    "        except (ValueError, AttributeError):\n",
    "            self.logger.warning(\"Could not parse price\")\n",
    "            return 0\n",
    "        return 0\n",
    "\n",
    "    def _parse_location(self, content: str) -> tuple[str, str]:\n",
    "        if not content or not self.districts:\n",
    "            return \"\", \"\"\n",
    "        content_lower = content.lower()\n",
    "        detected_district = next((d for d in self.districts if re.search(r\"\\b\" + re.escape(d.lower()) + r\"\\b\", content_lower)), \"\")\n",
    "        detected_ward = \"\"\n",
    "        if detected_district and detected_district in self.wards:\n",
    "            detected_ward = next((w for w in self.wards[detected_district] if re.search(r\"\\b\" + re.escape(w.lower()) + r\"\\b\", content_lower)), \"\")\n",
    "        return detected_district, detected_ward\n",
    "\n",
    "    def _parse_amenities(self, content: str) -> str:\n",
    "        if not content or not self.amenity_patterns:\n",
    "            return \"\"\n",
    "        content_lower = content.lower()\n",
    "        amenities = {label for label, pattern in self.amenity_patterns.items() if re.search(pattern, content_lower, re.IGNORECASE)}\n",
    "        return \", \".join(sorted(amenities))\n",
    "\n",
    "    def _parse_area(self, content: str) -> str:\n",
    "        if not content:\n",
    "            return \"\"\n",
    "        matches = re.findall(r'(\\d+(?:\\.\\d+)?)\\s*(?:m2|m²|met vuong)\\b', content, re.IGNORECASE)\n",
    "        return float(matches[0]) if matches else \"\"\n",
    "\n",
    "    def _parse_address(self, content: str, found_district: str, found_ward: str) -> str:\n",
    "        if not content:\n",
    "            return \"\"\n",
    "        \n",
    "        # Check explicit address patterns first\n",
    "        patterns = [r'(?:địa chỉ|đc|dc|tại|ở)[\\s:]+(\\d*\\s*[^\\n;]+)']\n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, content, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                address = match.strip()\n",
    "                if address.lower().startswith(\"là \"):\n",
    "                    address = address[3:].strip()\n",
    "\n",
    "                for district, streets in self.streets.items():\n",
    "                    for street in streets:\n",
    "                        if re.search(r'\\b' + re.escape(street) + r'\\b', address, re.IGNORECASE):\n",
    "\n",
    "                            street_match = re.search(r'(\\d*\\s*' + re.escape(street) + r'\\s*\\d*)', address, re.IGNORECASE)\n",
    "                            return street_match.group(0).strip() if street_match else street\n",
    "                return address  \n",
    "\n",
    "        for district, streets in self.streets.items():\n",
    "            for street in streets:\n",
    "                street_match = re.search(r'(\\d*\\s*' + re.escape(street) + r'\\s*\\d*)', content, re.IGNORECASE)\n",
    "                if street_match:\n",
    "                    return street_match.group(0).strip()\n",
    "        \n",
    "        return \"\"\n",
    "\n",
    "    def _parse_contact(self, content: str) -> str:\n",
    "        if not content:\n",
    "            return \"\"\n",
    "        pattern = r'\\b(?:0|\\+84)\\d{1,2}[\\s.-]?\\d{3,4}[\\s.-]?\\d{3,4}\\b'\n",
    "        matches = re.findall(pattern, content, re.IGNORECASE)\n",
    "        if not matches:\n",
    "            return \"\"\n",
    "        contact = matches[0].replace('O', '0').replace('o', '0')\n",
    "        contact = re.sub(r'[^\\d+]', '', contact)\n",
    "        if contact.startswith('+84') and 9 <= len(contact[3:]) <= 11:\n",
    "            return contact\n",
    "        if contact.startswith('0') and 9 <= len(contact) <= 11:\n",
    "            return contact\n",
    "        if contact.startswith('84') and 9 <= len(contact[2:]) <= 11:\n",
    "            return \"0\" + contact[2:]\n",
    "        if 9 <= len(contact) <= 10:\n",
    "            return \"0\" + contact\n",
    "        return \"\"\n",
    "\n",
    "    def parse_property_details(self, content):\n",
    "        if not content:\n",
    "            return {\n",
    "                \"area\": \"\", \"district\": \"\", \"ward\": \"\", \"address\": \"\",\n",
    "                \"amenities\": \"\", \"price\": 0, \"contact\": \"\"\n",
    "            }\n",
    "        price = self._parse_price(content)\n",
    "        district, ward = self._parse_location(content)\n",
    "        amenities = self._parse_amenities(content)\n",
    "        area = self._parse_area(content)\n",
    "        address = self._parse_address(content, district, ward)\n",
    "        contact = self._parse_contact(content)\n",
    "        return {\n",
    "            \"area\": area, \"district\": district, \"ward\": ward, \"address\": address,\n",
    "            \"amenities\": amenities, \"price\": price, \"contact\": contact\n",
    "        }\n",
    "\n",
    "    def load_existing_csv_data(self, csv_file_path):\n",
    "        existing_posts = []\n",
    "        content_hashes = set()\n",
    "        if os.path.exists(csv_file_path):\n",
    "            try:\n",
    "                with open(csv_file_path, 'r', encoding='utf-8', newline='') as f:\n",
    "                    csv_reader = csv.DictReader(f)\n",
    "                    for row in csv_reader:\n",
    "                        existing_posts.append(row)\n",
    "                        content_hashes.add(row.get(\"postID\", \"\"))\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error loading CSV: {e}\")\n",
    "        return existing_posts, content_hashes\n",
    "\n",
    "    def scrape_group_posts(self, group_url, max_posts, csv_file_path):\n",
    "        self.logger.info(f\"Scraping group: {group_url}\")\n",
    "        self.driver.get(group_url)\n",
    "        try:\n",
    "            WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"div.x1yztbdb.xh8yej3\")))\n",
    "        except TimeoutException:\n",
    "            self.logger.error(\"Posts did not load\")\n",
    "            return 0\n",
    "\n",
    "        csv_columns = [\"postID\", \"postDate\", \"content\", \"area\", \"district\", \"ward\", \"address\", \"amenities\", \"price\", \"contact\"]\n",
    "        all_posts, content_hashes = self.load_existing_csv_data(csv_file_path)\n",
    "        posts_scraped = 0\n",
    "\n",
    "        while posts_scraped < max_posts:\n",
    "            post_elements = self.driver.find_elements(By.CSS_SELECTOR, \"div.x1yztbdb.xh8yej3\")\n",
    "            new_posts = 0\n",
    "\n",
    "            for post in post_elements:\n",
    "                if posts_scraped >= max_posts:\n",
    "                    break\n",
    "                try:\n",
    "                    self.driver.execute_script(\"arguments[0].scrollIntoView(true);\", post)\n",
    "                    self.expand_post_content(post)\n",
    "                    content = self.extract_post_content(post)\n",
    "                    content_hash = self.generate_content_hash(content)\n",
    "                    if content_hash in content_hashes:\n",
    "                        continue\n",
    "                    content_hashes.add(content_hash)\n",
    "                    post_date = self.extract_post_date(post)\n",
    "                    property_details = self.parse_property_details(content)\n",
    "                    all_posts.append({\n",
    "                        \"postID\": content_hash, \"postDate\": post_date, \"content\": content,\n",
    "                        **property_details\n",
    "                    })\n",
    "                    posts_scraped += 1\n",
    "                    new_posts += 1\n",
    "                    self.logger.info(f\"Scraped post {posts_scraped}/{max_posts}\")\n",
    "                    time.sleep(random.uniform(1, 2))\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Error scraping post: {e}\")\n",
    "                    continue\n",
    "\n",
    "            if not new_posts:\n",
    "                break\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            try:\n",
    "                WebDriverWait(self.driver, 5).until(\n",
    "                    lambda d: len(d.find_elements(By.CSS_SELECTOR, \"div.x1yztbdb.x1n2onr6.xh8yej3.x1ja2u2z\")) > len(post_elements))\n",
    "            except TimeoutException:\n",
    "                break\n",
    "\n",
    "        try:\n",
    "            with open(csv_file_path, 'w', encoding='utf-8', newline='') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=csv_columns)\n",
    "                writer.writeheader()\n",
    "                writer.writerows(all_posts)\n",
    "            self.logger.info(f\"Saved {len(all_posts)} posts to {csv_file_path}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving CSV: {e}\")\n",
    "\n",
    "        return posts_scraped\n",
    "\n",
    "    def close(self):\n",
    "        try:\n",
    "            self.driver.quit()\n",
    "            self.logger.info(\"Browser closed\")\n",
    "        except Exception:\n",
    "            self.logger.info(\"No browser instance to close\")\n",
    "\n",
    "def main():\n",
    "    headless = False\n",
    "    cookies_file = \"facebook_cookies.json\"\n",
    "    config_file = \"config.json\"\n",
    "    max_posts = 10\n",
    "    csv_file_path = 'scrapData.csv'\n",
    "    groups = [\"https://www.facebook.com/groups/281184089051767\"]\n",
    "\n",
    "    scraper = FacebookGroupScraper(headless, cookies_file, config_file)\n",
    "    try:\n",
    "        if not scraper.login():\n",
    "            logging.error(\"Login failed\")\n",
    "            return\n",
    "        for group_url in groups:\n",
    "            posts_scraped = scraper.scrape_group_posts(group_url, max_posts, csv_file_path)\n",
    "            scraper.logger.info(f\"Scraped {posts_scraped} posts from {group_url}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Script error: {e}\")\n",
    "    finally:\n",
    "        scraper.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import FBdata to dtbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import hashlib\n",
    "import csv\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "from dotenv import load_dotenv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filemode='w',\n",
    "    filename='facebook_scraper.log'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def load_location_config(config_file):\n",
    "    \"\"\"Load district, ward, and amenities data from config file.\"\"\"\n",
    "    with open(config_file, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    districts = config.get('districts', [])\n",
    "    wards = config.get('wards', [])\n",
    "    amenities_keywords = config.get('amenities_keywords', [])\n",
    "\n",
    "    logger.info(\n",
    "        f\"Loaded {len(districts)} districts, {len(wards)} wards, and {len(amenities_keywords)} amenities from config\")\n",
    "\n",
    "    return districts, wards, amenities_keywords\n",
    "\n",
    "\n",
    "# Load location configuration\n",
    "try:\n",
    "    districts, wards, amenities_keywords = load_location_config(\"config.json\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load config: {str(e)}\")\n",
    "    # Default empty lists if config fails to load\n",
    "    districts, wards, amenities_keywords = [], [], []\n",
    "\n",
    "\n",
    "def generate_content_hash(content):\n",
    "    \"\"\"Generate a hash for post content to detect duplicates.\"\"\"\n",
    "    if not content:\n",
    "        return \"\"\n",
    "\n",
    "    # Normalize content by removing extra whitespace and lowercasing\n",
    "    normalized_content = ' '.join(content.lower().split())\n",
    "    return hashlib.md5(normalized_content.encode('utf-8')).hexdigest()\n",
    "\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Set up and configure the Chrome WebDriver.\"\"\"\n",
    "    try:\n",
    "        chrome_options = Options()\n",
    "        # Uncomment the following line to run headless\n",
    "        # chrome_options.add_argument('--headless')\n",
    "        chrome_options.add_argument('--disable-notifications')\n",
    "        chrome_options.add_argument('--disable-gpu')\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to set up WebDriver: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def login_facebook(driver, cookies_file):\n",
    "    \"\"\"Login to Facebook using saved cookies.\"\"\"\n",
    "    try:\n",
    "        # First load the Facebook site\n",
    "        driver.get(\"https://www.facebook.com\")\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Check if cookies file exists\n",
    "        if not os.path.exists(cookies_file):\n",
    "            logger.error(f\"Cookies file not found: {cookies_file}\")\n",
    "            return False\n",
    "\n",
    "        # Load cookies from file\n",
    "        try:\n",
    "            with open(cookies_file, \"r\") as file:\n",
    "                cookies = json.load(file)\n",
    "\n",
    "            # Add cookies to the driver\n",
    "            for cookie in cookies:\n",
    "                driver.add_cookie(cookie)\n",
    "\n",
    "            # Refresh to apply cookies\n",
    "            driver.refresh()\n",
    "            time.sleep(5)\n",
    "        except json.JSONDecodeError:\n",
    "            logger.error(\"Invalid JSON in cookies file\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading cookies: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "        # Verify login status\n",
    "        return verify_login_status(driver)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Login error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def verify_login_status(driver):\n",
    "    \"\"\"Verify if the login was successful by checking for key elements.\"\"\"\n",
    "    try:\n",
    "        success = False\n",
    "        selectors = [\n",
    "            (By.XPATH,\n",
    "             \"//div[@aria-label='Search Facebook' or @aria-label='Search']\"),\n",
    "            (By.CSS_SELECTOR, \"input[placeholder='Search Facebook']\"),\n",
    "            (By.CSS_SELECTOR, \"div[aria-label='Search']\"),\n",
    "            (By.XPATH,\n",
    "             \"//div[contains(@class, 'x1iorvi4') and contains(@class, 'x1pi30zi')]\"),\n",
    "            (By.XPATH, \"//span[text()='Home' or text()='Feed']\"),\n",
    "            (By.XPATH, \"//div[@role='navigation']\")\n",
    "        ]\n",
    "\n",
    "        for selector_type, selector in selectors:\n",
    "            try:\n",
    "                WebDriverWait(driver, 5).until(\n",
    "                    EC.presence_of_element_located((selector_type, selector))\n",
    "                )\n",
    "                success = True\n",
    "                logger.info(\n",
    "                    f\"Login successful - found element with selector: {selector}\")\n",
    "                break\n",
    "            except (TimeoutException, NoSuchElementException):\n",
    "                continue\n",
    "\n",
    "        if not success:\n",
    "            try:\n",
    "                login_button = driver.find_element(\n",
    "                    By.XPATH, \"//button[contains(text(), 'Log In') or contains(text(), 'Sign In')]\")\n",
    "                logger.error(\"Login failed - login button still present\")\n",
    "                return False\n",
    "            except NoSuchElementException:\n",
    "                logger.info(\"No login button found, assuming login successful\")\n",
    "                return True\n",
    "\n",
    "        return success\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during login verification: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def expand_post_content(driver, post_element):\n",
    "    \"\"\"Expand 'See more' buttons in post content.\"\"\"\n",
    "    try:\n",
    "        see_more_buttons = post_element.find_elements(\n",
    "            By.XPATH, \"//div[@role='button' and (text()='See more' or text()='See More')]\")\n",
    "        for btn in see_more_buttons:\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                time.sleep(0.5)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to click 'See more': {e}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error expanding post content: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def extract_post_url(post_element):\n",
    "    \"\"\"Extract clean post URL (remove query params) from post element.\"\"\"\n",
    "    try:\n",
    "        link = post_element.find_element(\n",
    "            By.XPATH, \".//div[contains(@class, 'xdj266r')]//a[@role='link' and contains(@href, '/posts/')]\"\n",
    "        )\n",
    "        full_url = link.get_attribute(\"href\")\n",
    "\n",
    "        if \"?\" in full_url:\n",
    "            clean_url = full_url.split(\"?\")[0]\n",
    "        else:\n",
    "            clean_url = full_url\n",
    "\n",
    "        return clean_url\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not extract post URL: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_post_date(post_element):\n",
    "    \"\"\"Extract post date from post element.\"\"\"\n",
    "    try:\n",
    "        date_element = post_element.find_element(\n",
    "            By.XPATH, \".//span[contains(@class,'x4k7w5x')]\"\n",
    "        )\n",
    "        return date_element.text\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to extract date: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_post_content(post_element):\n",
    "    \"\"\"Extract post content using multiple fallback methods.\"\"\"\n",
    "    try:\n",
    "        content_element = post_element.find_element(\n",
    "            By.XPATH, \".//div[@data-ad-rendering-role='story_message']\")\n",
    "        return content_element.text\n",
    "    except NoSuchElementException:\n",
    "        try:\n",
    "            content_element = post_element.find_element(\n",
    "                By.XPATH, \".//div[contains(@class, 'x6s0dn4') and contains(@class, 'xh8yej3')]\")\n",
    "            return content_element.text\n",
    "        except NoSuchElementException:\n",
    "            try:\n",
    "                content_element = post_element.find_element(\n",
    "                    By.XPATH, \".//div[@class='x1yztbdb x1n2onr6 xh8yej3 x1ja2u2z']\")\n",
    "                return content_element.text\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"No content extracted: {str(e)}\")\n",
    "                return \"\"\n",
    "\n",
    "\n",
    "def parse_property_details(content):\n",
    "    \"\"\"Parse property details from post content.\"\"\"\n",
    "    if not content:\n",
    "        return {\n",
    "            \"area\": \"\",\n",
    "            \"district\": \"\",\n",
    "            \"ward\": \"\",\n",
    "            \"address\": \"\",\n",
    "            \"amenities\": \"\",\n",
    "            \"price\": \"\",\n",
    "            \"contact\": \"\"\n",
    "        }\n",
    "\n",
    "    # Extract area\n",
    "    area_matches = re.findall(\n",
    "        r'(\\d+(?:\\.\\d+)?)\\s*(?:m2|m²|meter square|square meter)', content, re.IGNORECASE)\n",
    "    area = area_matches[0] if area_matches else \"\"\n",
    "\n",
    "    # Extract price\n",
    "    price_matches = re.findall(\n",
    "        r'(\\d+(?:\\.\\d+)?)\\s*(?:million|m|billion|b|tr|tỷ)\\s*(?:vnd|đ|vnđ)?', content, re.IGNORECASE)\n",
    "    price = price_matches[0] if price_matches else \"\"\n",
    "\n",
    "    # Extract district\n",
    "    found_district = \"\"\n",
    "    for d in districts:\n",
    "        if re.search(r'\\b' + re.escape(d) + r'\\b', content, re.IGNORECASE):\n",
    "            found_district = d\n",
    "            break\n",
    "\n",
    "    # If no district found from the list, try regex pattern as fallback\n",
    "    if not found_district:\n",
    "        district_matches = re.findall(\n",
    "            r'(?:district|quận|quan|huyen|huyện)\\s*(\\d+|[A-Za-z]+)', content, re.IGNORECASE)\n",
    "        found_district = district_matches[0] if district_matches else \"\"\n",
    "\n",
    "    # Extract ward\n",
    "    found_ward = \"\"\n",
    "    for w in wards:\n",
    "        if re.search(r'\\b' + re.escape(w) + r'\\b', content, re.IGNORECASE):\n",
    "            found_ward = w\n",
    "            break\n",
    "\n",
    "    # If no ward found from the list, try regex pattern as fallback\n",
    "    if not found_ward:\n",
    "        ward_matches = re.findall(\n",
    "            r'(?:ward|phường|phuong|phương|xã|xa)\\s*(\\d+|[A-Za-z]+)', content, re.IGNORECASE)\n",
    "        found_ward = ward_matches[0] if ward_matches else \"\"\n",
    "\n",
    "    # Extract address\n",
    "    address = \"\"\n",
    "    address_patterns = [\n",
    "        r'(?:address|địa chỉ|dia chi)[\\s:]+([^\\n]+)',\n",
    "        r'(?:located at|located in|at|in)[\\s:]+([^\\n]+)'\n",
    "    ]\n",
    "\n",
    "    for pattern in address_patterns:\n",
    "        address_matches = re.findall(pattern, content, re.IGNORECASE)\n",
    "        if address_matches:\n",
    "            address = address_matches[0].strip()\n",
    "            break\n",
    "\n",
    "    # If no structured address found, look for address format with district/ward\n",
    "    if not address and (found_district or found_ward):\n",
    "        lines = content.split('\\n')\n",
    "        for line in lines:\n",
    "            if (found_district and found_district.lower() in line.lower()) or \\\n",
    "               (found_ward and found_ward.lower() in line.lower()):\n",
    "                address = line.strip()\n",
    "                break\n",
    "\n",
    "    # Extract amenities\n",
    "    found_amenities = []\n",
    "    for keyword in amenities_keywords:\n",
    "        if re.search(r'\\b' + re.escape(keyword) + r'\\b', content, re.IGNORECASE):\n",
    "            found_amenities.append(keyword)\n",
    "\n",
    "    amenities = \", \".join(found_amenities)\n",
    "\n",
    "    # Extract contact info\n",
    "    phone_patterns = [\n",
    "        r'(?:\\+84|0)\\d{9,10}',  # Vietnamese phone numbers\n",
    "        r'(?:phone|tel|telephone|contact|số|sdt|số điện thoại)[:\\s]+(\\d[\\d\\s.-]{8,})'  # Labeled phone numbers\n",
    "    ]\n",
    "    \n",
    "    contact = \"\"\n",
    "    for pattern in phone_patterns:\n",
    "        matches = re.findall(pattern, content, re.IGNORECASE)\n",
    "        if matches:\n",
    "            contact = matches[0] if isinstance(matches[0], str) else matches[0][0]\n",
    "            contact = re.sub(r'\\s+', '', contact) \n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"area\": area,\n",
    "        \"district\": found_district,\n",
    "        \"ward\": found_ward,\n",
    "        \"address\": address,\n",
    "        \"amenities\": amenities,\n",
    "        \"price\": price,\n",
    "        \"contact\": contact\n",
    "    }\n",
    "\n",
    "\n",
    "def load_existing_csv_data(csv_file_path):\n",
    "    \"\"\"Load existing data from CSV file if it exists.\"\"\"\n",
    "    existing_posts = []\n",
    "    processed_urls = set()\n",
    "    content_hashes = set()\n",
    "    \n",
    "    if os.path.exists(csv_file_path):\n",
    "        try:\n",
    "            with open(csv_file_path, 'r', encoding='utf-8', newline='') as f:\n",
    "                csv_reader = csv.DictReader(f)\n",
    "                for row in csv_reader:\n",
    "                    existing_posts.append(row)\n",
    "                    post_id = row.get(\"postID\", \"\")\n",
    "                    if post_id:\n",
    "                        content_hashes.add(post_id)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading existing CSV file: {str(e)}\")\n",
    "    \n",
    "    return existing_posts, processed_urls, content_hashes\n",
    "\n",
    "\n",
    "def scrape_facebook_group(driver, group_url, max_posts, csv_file_path):\n",
    "    \"\"\"Scrape posts from Facebook group and save to CSV.\"\"\"\n",
    "    driver.get(group_url)\n",
    "    time.sleep(3)  # Allow page to load\n",
    "\n",
    "    # Define CSV columns\n",
    "    csv_columns = [\n",
    "        \"postID\", \"postDate\", \"content\",\n",
    "        \"area\", \"district\", \"ward\", \"address\", \"amenities\", \n",
    "        \"price\", \"contact\"\n",
    "    ]\n",
    "\n",
    "    # Load existing data if file exists\n",
    "    all_posts, processed_urls, content_hashes = load_existing_csv_data(csv_file_path)\n",
    "    # Convert list of dictionaries to dictionary with postID as key for easy updating\n",
    "    posts_dict = {post.get(\"postID\", \"\"): post for post in all_posts if post.get(\"postID\", \"\")}\n",
    "\n",
    "    posts_scraped = 0\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while posts_scraped < max_posts:\n",
    "        # Find all post elements - refresh this query after each scroll\n",
    "        post_elements = driver.find_elements(\n",
    "            By.XPATH, \".//div[@class='x1yztbdb x1n2onr6 xh8yej3 x1ja2u2z']\"\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Found {len(post_elements)} post elements\")\n",
    "\n",
    "        # Keep track of how many new posts we processed in this batch\n",
    "        new_posts_in_batch = 0\n",
    "\n",
    "        # Process each post element individually\n",
    "        for post in post_elements:\n",
    "            try:\n",
    "                # Scroll to the post to ensure it's in view\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView(true);\", post)\n",
    "                time.sleep(1)  # Allow content to load\n",
    "\n",
    "                # Expand post content\n",
    "                expand_post_content(driver, post)\n",
    "\n",
    "                # Extract post URL first to check if we've already processed this post\n",
    "                post_url = extract_post_url(post)\n",
    "\n",
    "                # Skip if already processed by URL (but not in content_hashes to allow updates)\n",
    "                if post_url in processed_urls:\n",
    "                    logger.info(f\"Skipping already processed post: {post_url}\")\n",
    "                    continue\n",
    "\n",
    "                # Extract content and generate hash\n",
    "                content = extract_post_content(post)\n",
    "                content_hash = generate_content_hash(content)\n",
    "\n",
    "                # Mark URL as processed\n",
    "                processed_urls.add(post_url)\n",
    "\n",
    "                # Extract other data\n",
    "                post_date = extract_post_date(post)\n",
    "\n",
    "                # Parse property details\n",
    "                property_details = parse_property_details(content)\n",
    "\n",
    "                # Create post data structure\n",
    "                post_data = {\n",
    "                    \"postID\": content_hash,\n",
    "                    \"postDate\": post_date,\n",
    "                    \"content\": content,\n",
    "                    \"area\": property_details[\"area\"],\n",
    "                    \"district\": property_details[\"district\"],\n",
    "                    \"ward\": property_details[\"ward\"],\n",
    "                    \"address\": property_details[\"address\"],\n",
    "                    \"amenities\": property_details[\"amenities\"],\n",
    "                    \"price\": property_details[\"price\"],\n",
    "                    \"contact\": property_details[\"contact\"]\n",
    "                }\n",
    "\n",
    "                # Update or add to posts dictionary\n",
    "                if content_hash in posts_dict:\n",
    "                    logger.info(f\"Updating existing post with ID: {content_hash}\")\n",
    "                    posts_dict[content_hash] = post_data\n",
    "                else:\n",
    "                    logger.info(f\"Adding new post with ID: {content_hash}\")\n",
    "                    posts_dict[content_hash] = post_data\n",
    "                    content_hashes.add(content_hash)\n",
    "\n",
    "                posts_scraped += 1\n",
    "                new_posts_in_batch += 1\n",
    "                logger.info(f\"Scraped post {posts_scraped}/{max_posts}\")\n",
    "\n",
    "                # Exit the loop if we've scraped enough posts\n",
    "                if posts_scraped >= max_posts:\n",
    "                    break\n",
    "\n",
    "                # Add random delay to avoid detection\n",
    "                time.sleep(random.uniform(1.5, 3.0))\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error scraping post: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        # If we didn't find any new posts in this batch, scroll down\n",
    "        if new_posts_in_batch == 0:\n",
    "            logger.info(\"No new posts found in current view, scrolling down...\")\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(3)  # Wait for new content to load\n",
    "\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                # If no new content loaded, break out of the loop\n",
    "                logger.warning(\"No more posts to load\")\n",
    "                break\n",
    "\n",
    "            last_height = new_height\n",
    "        elif posts_scraped < max_posts:\n",
    "            # If we found posts but need more, scroll down to load more\n",
    "            logger.info(f\"Found {new_posts_in_batch} new posts, scrolling down to get more...\")\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(3)  # Wait for new content to load\n",
    "\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                logger.warning(\"No more posts to load\")\n",
    "                break\n",
    "\n",
    "            last_height = new_height\n",
    "\n",
    "    # Convert dictionary back to list for CSV export\n",
    "    updated_posts = list(posts_dict.values())\n",
    "\n",
    "    # Save all posts to CSV file\n",
    "    try:\n",
    "        with open(csv_file_path, 'w', encoding='utf-8', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=csv_columns)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(updated_posts)\n",
    "        logger.info(f\"Successfully saved {len(updated_posts)} posts to {csv_file_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving to CSV file: {str(e)}\")\n",
    "\n",
    "    return posts_scraped, updated_posts\n",
    "\n",
    "\n",
    "def export_to_database(csv_file_path):\n",
    "    \"\"\"Export data from CSV to MySQL database with update/insert logic.\"\"\"\n",
    "    logger.info(f\"Starting database export from {csv_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Load CSV into DataFrame\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        logger.info(f\"Loaded {len(df)} rows from CSV\")\n",
    "        \n",
    "        # Connect to database\n",
    "        conn = mysql.connector.connect(\n",
    "            host=os.getenv('db_host'),\n",
    "            user=os.getenv('db_user'),\n",
    "            password=os.getenv('db_password'),\n",
    "            database=os.getenv('db_name'),\n",
    "            connection_timeout=10\n",
    "        )\n",
    "        \n",
    "        conn.autocommit = False  # Disable autocommit for batch processing\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Prepare SQL statements for insert and update\n",
    "        insert_sql = \"\"\"\n",
    "            INSERT INTO post (\n",
    "                postID, p_date, content, district, ward,\n",
    "                street_address, price, area, amenities, contact_info\n",
    "            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        \n",
    "        update_sql = \"\"\"\n",
    "            UPDATE post SET\n",
    "                p_date = %s,\n",
    "                content = %s,\n",
    "                district = %s,\n",
    "                ward = %s,\n",
    "                street_address = %s,\n",
    "                price = %s,\n",
    "                area = %s,\n",
    "                amenities = %s,\n",
    "                contact_info = %s\n",
    "            WHERE postID = %s\n",
    "        \"\"\"\n",
    "        \n",
    "        # Check which records already exist in the database\n",
    "        existing_ids_query = \"SELECT postID FROM post\"\n",
    "        cursor.execute(existing_ids_query)\n",
    "        existing_ids = set(row[0] for row in cursor.fetchall())\n",
    "        \n",
    "        BATCH_SIZE = 100\n",
    "        retry_limit = 3\n",
    "        inserts = 0\n",
    "        updates = 0\n",
    "        \n",
    "        for i, row in df.iterrows():\n",
    "            post_id = None if pd.isna(row[\"postID\"]) else str(row[\"postID\"])\n",
    "            \n",
    "            if not post_id:\n",
    "                continue  # Skip rows without a valid post ID\n",
    "                \n",
    "            try:\n",
    "                # Prepare values tuple\n",
    "                values = (\n",
    "                    None if pd.isna(row[\"postDate\"]) else row[\"postDate\"],\n",
    "                    None if pd.isna(row[\"content\"]) else row[\"content\"],\n",
    "                    None if pd.isna(row[\"district\"]) else row[\"district\"],\n",
    "                    None if pd.isna(row[\"ward\"]) else row[\"ward\"],\n",
    "                    None if pd.isna(row[\"address\"]) else row[\"address\"],\n",
    "                    None if pd.isna(row[\"price\"]) else float(row[\"price\"]),\n",
    "                    None if pd.isna(row[\"area\"]) else float(row[\"area\"]) if row[\"area\"] and row[\"area\"].replace('.', '').isdigit() else None,\n",
    "                    None if pd.isna(row[\"amenities\"]) else row[\"amenities\"],\n",
    "                    None if pd.isna(row[\"contact\"]) else row[\"contact\"]\n",
    "                )\n",
    "                \n",
    "                # Retry mechanism for database operations\n",
    "                for attempt in range(retry_limit):\n",
    "                    try:\n",
    "                        if post_id in existing_ids:\n",
    "                            # Update existing record (append post_id to the end of values)\n",
    "                            cursor.execute(update_sql, values + (post_id,))\n",
    "                            updates += 1\n",
    "                        else:\n",
    "                            # Insert new record (prepend post_id to the values)\n",
    "                            cursor.execute(insert_sql, (post_id,) + values)\n",
    "                            existing_ids.add(post_id)  # Add to existing IDs set\n",
    "                            inserts += 1\n",
    "                        break  # success\n",
    "                    except mysql.connector.errors.DatabaseError as e:\n",
    "                        if \"Lock wait timeout exceeded\" in str(e):\n",
    "                            logger.warning(f\"Lock timeout on row {i}, retrying ({attempt + 1}/{retry_limit})...\")\n",
    "                            time.sleep(2)\n",
    "                        else:\n",
    "                            raise\n",
    "                \n",
    "                # Commit batch\n",
    "                if i % BATCH_SIZE == 0 and i > 0:\n",
    "                    conn.commit()\n",
    "                    logger.info(f\"Committed batch, processed {i} rows (inserts: {inserts}, updates: {updates})\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing row {i}: {e}\")\n",
    "        \n",
    "        # Final commit\n",
    "        conn.commit()\n",
    "        logger.info(f\"Database export complete. Total: {len(df)} rows, {inserts} inserts, {updates} updates\")\n",
    "        \n",
    "        return inserts, updates\n",
    "    \n",
    "    except Error as e:\n",
    "        logger.error(f\"Database error: {e}\")\n",
    "        if 'conn' in locals() and conn.is_connected():\n",
    "            conn.rollback()\n",
    "        return 0, 0\n",
    "    except Exception as e:\n",
    "        logger.error(f\"General error during database export: {e}\")\n",
    "        if 'conn' in locals() and conn.is_connected():\n",
    "            conn.rollback()\n",
    "        return 0, 0\n",
    "    finally:\n",
    "        if 'conn' in locals() and conn.is_connected():\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            logger.info(\"Database connection closed\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the scraper and database export.\"\"\"\n",
    "    csv_file_path = 'phongtro_data.csv'\n",
    "    \n",
    "    try:\n",
    "        # Set up WebDriver\n",
    "        driver = setup_driver()\n",
    "\n",
    "        # Login to Facebook\n",
    "        if login_facebook(driver, \"facebook_cookies.json\"):\n",
    "            logger.info(\"Login successful\")\n",
    "\n",
    "            # Define Facebook groups to scrape\n",
    "            groups = [\"https://www.facebook.com/groups/281184089051767\"]\n",
    "            \n",
    "            total_scraped = 0\n",
    "            \n",
    "            # Scrape each group\n",
    "            for group_url in groups:\n",
    "                logger.info(f\"Scraping group: {group_url}\")\n",
    "                posts_scraped, scraped_posts = scrape_facebook_group(\n",
    "                    driver, group_url, max_posts=10, csv_file_path=csv_file_path)\n",
    "                total_scraped += posts_scraped\n",
    "                logger.info(f\"Scraped {posts_scraped} posts from group {group_url}\")\n",
    "            \n",
    "            if total_scraped > 0 or os.path.exists(csv_file_path):\n",
    "                logger.info(\"Starting database export...\")\n",
    "                inserts, updates = export_to_database(csv_file_path)\n",
    "                logger.info(f\"Database export completed: {inserts} new records, {updates} updated records\")\n",
    "            else:\n",
    "                logger.warning(\"No data to export to database\")\n",
    "        else:\n",
    "            logger.error(\"Failed to login to Facebook\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Script error: {str(e)}\")\n",
    "    finally:\n",
    "        if 'driver' in locals():\n",
    "            driver.quit()\n",
    "            logger.info(\"Browser closed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import hashlib\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    NoSuchElementException, TimeoutException, StaleElementReferenceException\n",
    ")\n",
    "\n",
    "\n",
    "# ======== CONFIGURATION ========\n",
    "DEFAULT_CONFIG = {\n",
    "    \"city\": \"da-nang\",           # City to scrape data from (URL path)\n",
    "    \"post_limit\": 0,             # Number of posts to scrape (0 = all)\n",
    "    \"output_file\": \"phongtro_test.csv\",  # Output filename\n",
    "    \"headless\": True,            # Run browser in headless mode (True) or visible (False)\n",
    "    \"random_delay\": True,        # Add random delay between operations (to avoid blocking)\n",
    "    \"min_delay\": 1,              # Minimum delay (seconds)\n",
    "    \"max_delay\": 3,              # Maximum delay (seconds)\n",
    "}\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filemode='w',\n",
    "    filename='phongtro_scraper.log'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class WebScraper:\n",
    "    def __init__(self, config: Dict[str, Any] = None):\n",
    "        \"\"\"Initialize scraper with configuration.\"\"\"\n",
    "        self.config = config or DEFAULT_CONFIG\n",
    "        self.driver = None\n",
    "        self.patterns = self._load_config()\n",
    "        \n",
    "    def _load_config(self) -> Dict:\n",
    "        \"\"\"Load patterns and location data from config.json file.\"\"\"\n",
    "        try:\n",
    "            with open('config.json', 'r', encoding='utf-8') as config_file:\n",
    "                return json.load(config_file)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading config.json: {str(e)}\")\n",
    "            return {}\n",
    "    \n",
    "    def setup_driver(self) -> webdriver.Edge:\n",
    "        \"\"\"Set up and return WebDriver instance.\"\"\"\n",
    "        options = Options()\n",
    "        if self.config[\"headless\"]:\n",
    "            options.add_argument(\"--headless\")  \n",
    "            options.add_argument(\"--disable-gpu\")\n",
    "        \n",
    "        options.add_argument(\"--window-size=720,1080\")\n",
    "        options.add_argument(\"--disable-notifications\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        \n",
    "        # Add user-agent to avoid detection as bot\n",
    "        options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\")\n",
    "        \n",
    "        self.driver = webdriver.Edge(options=options)\n",
    "        return self.driver\n",
    "\n",
    "    def random_delay(self) -> float:\n",
    "        \"\"\"Create random delay if configured.\"\"\"\n",
    "        if self.config[\"random_delay\"]:\n",
    "            delay = random.uniform(self.config[\"min_delay\"], self.config[\"max_delay\"])\n",
    "            time.sleep(delay)\n",
    "            return delay\n",
    "        return 0\n",
    "\n",
    "    def check_and_move_to_next_page(self) -> bool:\n",
    "        \"\"\"Check if 'Next page' button is available and click it.\"\"\"\n",
    "        try:\n",
    "            next_button = self.driver.find_element(By.XPATH, \"//a[text()='Trang sau »']\")\n",
    "            if next_button.is_enabled():\n",
    "                next_button.click()\n",
    "                delay = self.random_delay()\n",
    "                logger.info(f\"Moved to next page (waited {delay:.2f}s)\")\n",
    "                return True\n",
    "            else:\n",
    "                logger.info(\"'Next' button not available or not found.\")\n",
    "                return False\n",
    "        except NoSuchElementException:\n",
    "            logger.info(\"'Next' button not found on this page.\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error moving to next page: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def get_all_urls(self, max_posts: int = 0) -> List[str]:\n",
    "        \"\"\"Get all post URLs from the website, limit if specified.\"\"\"\n",
    "        all_post_url = []\n",
    "        current_page = 1\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                logger.info(f\"Getting URLs from page {current_page}\")\n",
    "                # Wait for elements to load\n",
    "                WebDriverWait(self.driver, 10).until(\n",
    "                    EC.presence_of_all_elements_located((By.XPATH, \"//a[contains(@class,'line-clamp-2')]\"))\n",
    "                )\n",
    "                \n",
    "                post_elements = self.driver.find_elements(By.XPATH, \"//a[contains(@class,'line-clamp-2')]\")\n",
    "                \n",
    "                for element in post_elements:\n",
    "                    url = element.get_attribute('href')\n",
    "                    all_post_url.append(url)\n",
    "                    logger.debug(f\"Added URL: {url}\")\n",
    "                    \n",
    "                    # Check limit\n",
    "                    if max_posts > 0 and len(all_post_url) >= max_posts:\n",
    "                        logger.info(f\"Reached limit of {max_posts} posts.\")\n",
    "                        return all_post_url[:max_posts]\n",
    "                \n",
    "                logger.info(f\"Collected {len(all_post_url)} URLs\")\n",
    "                \n",
    "                # Go to next page if available\n",
    "                if not self.check_and_move_to_next_page():\n",
    "                    break\n",
    "                    \n",
    "                current_page += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error getting URLs: {str(e)}\")\n",
    "                break\n",
    "        \n",
    "        return all_post_url\n",
    "\n",
    "    def extract_datetime(self, date_time_str: str) -> str:\n",
    "        \"\"\"Extract date and time from string and format it as 'YYYY-MM-DD HH:MM:SS'.\"\"\"\n",
    "        try:\n",
    "            parts = date_time_str.split(', ')\n",
    "            if len(parts) < 2:\n",
    "                return \"\"\n",
    "\n",
    "            raw_datetime = parts[1] \n",
    "            dt = datetime.strptime(raw_datetime, \"%H:%M %d/%m/%Y\")\n",
    "            return dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting date and time: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    def get_post_content(self) -> str:\n",
    "        \"\"\"Get post content from description.\"\"\"\n",
    "        try:\n",
    "            # Wait for element to load\n",
    "            WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//div[@class='border-bottom pb-3 mb-4']\"))\n",
    "            )\n",
    "            \n",
    "            description_container = self.driver.find_element(By.XPATH, \"//div[@class='border-bottom pb-3 mb-4']\")\n",
    "            paragraphs = description_container.find_elements(By.XPATH, \"./p\")\n",
    "            \n",
    "            post_content_paragraphs = [p.text.strip() for p in paragraphs]\n",
    "            post_content = \"\\n\".join(post_content_paragraphs)\n",
    "            return post_content.strip()  \n",
    "\n",
    "        except TimeoutException:\n",
    "            logger.warning(\"Timeout waiting for description element\")\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting post content: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    def generate_post_id(self, content: str) -> str:\n",
    "        \"\"\"Generate unique ID from post content.\"\"\"\n",
    "        return hashlib.md5(content.encode('utf-8')).hexdigest()\n",
    "\n",
    "    def get_district_and_ward(self, address: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "        \"\"\"Extract district and ward from address string using keyword matching.\"\"\"\n",
    "        if not address or not self.patterns:\n",
    "            return None, None\n",
    "\n",
    "        try:\n",
    "            # Get district list from config\n",
    "            districts = self.patterns.get(\"districts\", [])\n",
    "            wards = self.patterns.get(\"wards\", {})\n",
    "\n",
    "            detected_district = None\n",
    "            for district in districts:\n",
    "                if re.search(r\"\\b\" + re.escape(district) + r\"\\b\", address, re.IGNORECASE):\n",
    "                    detected_district = district\n",
    "                    break\n",
    "\n",
    "            detected_ward = None\n",
    "            if detected_district and detected_district in wards:\n",
    "                for ward in wards[detected_district]:\n",
    "                    if re.search(r\"\\b\" + re.escape(ward) + r\"\\b\", address, re.IGNORECASE):\n",
    "                        detected_ward = ward\n",
    "                        break\n",
    "\n",
    "            # Fall back to simple substring match if regex fails\n",
    "            if not detected_ward and detected_district:\n",
    "                address_lower = address.lower()\n",
    "                for ward in wards[detected_district]:\n",
    "                    if ward.lower() in address_lower:\n",
    "                        detected_ward = ward\n",
    "                        break\n",
    "\n",
    "            return detected_district, detected_ward\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing address '{address}': {str(e)}\")\n",
    "            return None, None\n",
    "        \n",
    "    def get_amenities(self, content: str) -> List[str]:\n",
    "        \"\"\"Get amenities list from post.\"\"\"\n",
    "        if not self.patterns:\n",
    "            return []\n",
    "\n",
    "        # Get amenity patterns from config\n",
    "        amenity_patterns = self.patterns.get(\"amenity_patterns\", {})\n",
    "        detected_amenities = set()\n",
    "\n",
    "        try:\n",
    "            WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((\n",
    "                    By.XPATH,\n",
    "                    \"//div[@class='text-body d-flex pt-1 pb-1' and not(contains(@style, '--bs-text-opacity: 0.1;'))]\"\n",
    "                ))\n",
    "            )\n",
    "\n",
    "            amenity_elements = self.driver.find_elements(\n",
    "                By.XPATH,\n",
    "                \"//div[@class='text-body d-flex pt-1 pb-1' and not(contains(@style, '--bs-text-opacity: 0.1;'))]\"\n",
    "            )\n",
    "\n",
    "            for element in amenity_elements:\n",
    "                text = element.text.strip()\n",
    "                if text:\n",
    "                    matched = False\n",
    "                    for label, pattern in amenity_patterns.items():\n",
    "                        if re.search(pattern, text, re.IGNORECASE):\n",
    "                            detected_amenities.add(label)\n",
    "                            matched = True\n",
    "                            break\n",
    "                    if not matched:\n",
    "                        detected_amenities.add(text) \n",
    "\n",
    "            # Get from content\n",
    "            for label, pattern in amenity_patterns.items():\n",
    "                if re.search(pattern, content, re.IGNORECASE):\n",
    "                    detected_amenities.add(label)\n",
    "\n",
    "            return list(detected_amenities)\n",
    "\n",
    "        except TimeoutException:\n",
    "            logger.warning(\"Timeout waiting for amenity elements\")\n",
    "            return list(detected_amenities)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting amenities: {str(e)}\")\n",
    "            return list(detected_amenities)\n",
    "\n",
    "    def extract_price_value(self, price_str: str) -> Optional[int]:\n",
    "        \"\"\"Extract numeric value from price string and return as integer (VND).\"\"\"\n",
    "        try:\n",
    "            s = price_str.lower().replace('đồng', '').replace('vnd', '').replace('/tháng', '').strip()\n",
    "\n",
    "            if m := re.search(r'(\\d+)[.,](\\d+)\\s*triệu', s):\n",
    "                return int(m.group(1)) * 1_000_000 + int(m.group(2).ljust(2, '0')) * 10_000\n",
    "            elif m := re.search(r'(\\d+)\\s*triệu', s):\n",
    "                return int(m.group(1)) * 1_000_000\n",
    "            elif m := re.search(r'(\\d{3,}(?:[.,]\\d{3})*)', s):\n",
    "                return int(m.group(1).replace('.', '').replace(',', ''))\n",
    "\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing price: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_area_value(self, area_str: str) -> Optional[float | int]:\n",
    "        \"\"\"Extract numeric value from area string. Return int if whole number, else float.\"\"\"\n",
    "        try:\n",
    "            match = re.search(r'([\\d.,]+)', area_str)\n",
    "            if not match:\n",
    "                return None\n",
    "\n",
    "            number = float(match.group(1).replace(',', '.'))\n",
    "            return int(number) if number.is_integer() else number\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing area: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "        \n",
    "    def get_element_text_safely(self, xpath: str, default: str = \"\") -> str:\n",
    "        \"\"\"Safely get text from an element with fallback.\"\"\"\n",
    "        try:\n",
    "            element = self.driver.find_element(By.XPATH, xpath)\n",
    "            return element.text.strip()\n",
    "        except (NoSuchElementException, StaleElementReferenceException):\n",
    "            return default\n",
    "        \n",
    "    def _extract_metadata(self) -> Dict[str, Any]:\n",
    "        raw = self.get_element_text_safely(\n",
    "            \"(//td[@class='border-0 pb-0'])[2]\",\n",
    "            self.get_element_text_safely(\"(//table[@class='table table-borderless align-middle m-0'])/tbody//tr[5]\")\n",
    "        )\n",
    "        return {\n",
    "            \"time\": self.extract_datetime(raw)\n",
    "        }\n",
    "\n",
    "    def _extract_address_and_location(self) -> Dict[str, Any]:\n",
    "        address = self.get_element_text_safely(\n",
    "            \"(//td[@colspan='3'])[3]\",\n",
    "            self.get_element_text_safely(\"(//table[@class='table table-borderless align-middle m-0'])/tbody//tr[3]/td[2]\")\n",
    "        )\n",
    "        district, ward = self.get_district_and_ward(address)\n",
    "        return {\n",
    "            \"address\": address,\n",
    "            \"district\": district,\n",
    "            \"ward\": ward\n",
    "        }\n",
    "\n",
    "    def _extract_price_area(self, content: str) -> Dict[str, Any]:\n",
    "        price_str = self.get_element_text_safely(\n",
    "            \"//span[@class='text-price fs-5 fw-bold']\",\n",
    "            self.get_element_text_safely(\"//span[@class='text-green fs-5 fw-bold']\")\n",
    "        )\n",
    "        area_str = self.get_element_text_safely(\"//div[@class='d-flex justify-content-between']/div/span[3]\")\n",
    "        return {\n",
    "            \"price\": self.extract_price_value(price_str),\n",
    "            \"area\": self.extract_area_value(area_str),\n",
    "            \"amenities\": self.get_amenities(content)\n",
    "        }\n",
    "\n",
    "    def _extract_contact(self) -> str:\n",
    "        return self.get_element_text_safely(\n",
    "            \"//div[@class='mb-4']//i[@class='icon telephone-fill white me-2']/..\"\n",
    "        ).strip()\n",
    "\n",
    "    def get_post_data(self, url: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Get post data from URL by extracting parts separately.\"\"\"\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            delay = self.random_delay()\n",
    "            logger.info(f\"Loading page {url} (waited {delay:.2f}s)\")\n",
    "\n",
    "            if \"Page not found\" in self.driver.title or \"Error\" in self.driver.title:\n",
    "                logger.warning(f\"Page doesn't exist or has error: {url}\")\n",
    "                return None\n",
    "\n",
    "            content = self.get_post_content()\n",
    "            post_id = self.generate_post_id(content)\n",
    "\n",
    "            metadata = self._extract_metadata()\n",
    "            address_data = self._extract_address_and_location()\n",
    "            pricing = self._extract_price_area(content)\n",
    "            contact = self._extract_contact()\n",
    "\n",
    "            return {\n",
    "                \"postID\": post_id,\n",
    "                \"time\": metadata[\"time\"],\n",
    "                \"content\": content,\n",
    "                \"address\": address_data[\"address\"],\n",
    "                \"ward\": address_data[\"ward\"],\n",
    "                \"district\": address_data[\"district\"],\n",
    "                \"area\": pricing[\"area\"],\n",
    "                \"price\": pricing[\"price\"],\n",
    "                \"amenities\": \", \".join(pricing[\"amenities\"]),  # Join list for CSV\n",
    "                \"contact\": contact,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting data from URL {url}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def save_to_csv(self, data: List[Dict], filename: str) -> bool:\n",
    "        \"\"\"Save data to CSV file.\"\"\"\n",
    "        try:\n",
    "            if not data:\n",
    "                logger.warning(\"No data to save to CSV.\")\n",
    "                return False\n",
    "                \n",
    "            # Get field names from the first item\n",
    "            fieldnames = list(data[0].keys())\n",
    "            \n",
    "            with open(filename, 'w', encoding='utf-8', newline='') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                for row in data:\n",
    "                    writer.writerow(row)\n",
    "            \n",
    "            logger.info(f\"Saved data to {filename}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving data to CSV file: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def print_summary(self, post_data_list: List[Dict]):\n",
    "        \"\"\"Print summary of collected data.\"\"\"\n",
    "        if not post_data_list:\n",
    "            print(\"No data collected!\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"🏠 PHONGTRO DATA COLLECTION SUMMARY 🏠\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"✅ Number of posts collected: {len(post_data_list)}\")\n",
    "        \n",
    "        # District stats\n",
    "        districts = {}\n",
    "        for post in post_data_list:\n",
    "            district = post.get(\"district\", \"\")\n",
    "            if district:\n",
    "                districts[district] = districts.get(district, 0) + 1\n",
    "        \n",
    "        if districts:\n",
    "            print(\"\\n📍 Distribution by district:\")\n",
    "            for district, count in sorted(districts.items(), key=lambda x: x[1], reverse=True):\n",
    "                print(f\"  • District {district}: {count} posts\")\n",
    "        \n",
    "        # Price stats\n",
    "        prices = [post.get(\"price\", \"\") for post in post_data_list if post.get(\"price\")]\n",
    "        if prices:\n",
    "            print(\"\\n💰 Price information:\")\n",
    "            print(f\"  • Number of posts with price info: {len(prices)}\")\n",
    "        \n",
    "        print(\"\\n💾 Data saved to: \" + self.config[\"output_file\"])\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    def collect_posts(self, urls: List[str]) -> List[Dict[str, Any]]:\n",
    "        posts = []\n",
    "        for i, url in enumerate(urls):\n",
    "            print(f\"Processing post {i+1}/{len(urls)}\", end='\\r')\n",
    "            logger.info(f\"Processing {i+1}/{len(urls)}: {url}\")\n",
    "            data = self.get_post_data(url)\n",
    "            if data:\n",
    "                posts.append(data)\n",
    "        return posts\n",
    "\n",
    "    def run(self):\n",
    "        self._print_header()\n",
    "        if not self.patterns:\n",
    "            print(\"Error: Could not load config.json.\")\n",
    "            return\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.setup_driver()\n",
    "\n",
    "        try:\n",
    "            self.driver.get(f\"https://phongtro123.com/tinh-thanh/{self.config['city']}?orderby=moi-nhat\")\n",
    "            self.random_delay()\n",
    "\n",
    "            urls = self.get_all_urls(self.config[\"post_limit\"])\n",
    "            posts = self.collect_posts(urls)\n",
    "\n",
    "            if posts:\n",
    "                self.save_to_csv(posts, self.config[\"output_file\"])\n",
    "                self.print_summary(posts)\n",
    "            else:\n",
    "                print(\"No data collected.\")\n",
    "        finally:\n",
    "            self.driver.quit()\n",
    "            print(f\"⏱️ Execution time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    \n",
    "    def _print_header(self):\n",
    "        \"\"\"Print program header.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"🏠 PHONGTRO DATA SCRAPER 🏠\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"• City: {self.config['city']}\")\n",
    "        print(f\"• Post limit: {self.config['post_limit'] if self.config['post_limit'] > 0 else 'No limit'}\")\n",
    "        print(f\"• Output file: {self.config['output_file']}\")\n",
    "        print(f\"• Headless mode: {'On' if self.config['headless'] else 'Off'}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = WebScraper(DEFAULT_CONFIG)\n",
    "    scraper.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import webdata to dtbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import hashlib\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    NoSuchElementException, TimeoutException, StaleElementReferenceException\n",
    ")\n",
    "\n",
    "\n",
    "# ======== CONFIGURATION ========\n",
    "DEFAULT_CONFIG = {\n",
    "    \"city\": \"da-nang\",                      # City to scrape data from (URL path)\n",
    "    \"post_limit\": 0,                        # Number of posts to scrape (0 = all)\n",
    "    \"output_file\": \"Scraped_data.csv\",      # Output filename\n",
    "    \"headless\": True,                       # Run browser in headless mode (True) or visible (False)\n",
    "    \"random_delay\": True,                   # Add random delay between operations (to avoid blocking)\n",
    "    \"min_delay\": 1,                         # Minimum delay (seconds)\n",
    "    \"max_delay\": 3,                         # Maximum delay (seconds)\n",
    "    \"import_to_db\": True,                   # Import data to database after scraping\n",
    "    \"db_batch_size\": 100,                   # Number of records to commit in each batch\n",
    "    \"db_retry_limit\": 3,                    # Number of retries for database operations\n",
    "}\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filemode='w',\n",
    "    filename='data_scraper.log'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class DateScraper:\n",
    "    def __init__(self, config: Dict[str, Any] = None):\n",
    "        \"\"\"Initialize scraper with configuration.\"\"\"\n",
    "        self.config = config or DEFAULT_CONFIG\n",
    "        self.driver = None\n",
    "        self.patterns = self._load_config()\n",
    "        self.db_connection = None\n",
    "        self.db_cursor = None\n",
    "        \n",
    "    def _load_config(self) -> Dict:\n",
    "        \"\"\"Load patterns and location data from config.json file.\"\"\"\n",
    "        try:\n",
    "            with open('config.json', 'r', encoding='utf-8') as config_file:\n",
    "                return json.load(config_file)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading config.json: {str(e)}\")\n",
    "            return {}\n",
    "    \n",
    "    def setup_driver(self) -> webdriver.Edge:\n",
    "        \"\"\"Set up and return WebDriver instance.\"\"\"\n",
    "        options = Options()\n",
    "        if self.config[\"headless\"]:\n",
    "            options.add_argument(\"--headless\")  \n",
    "            options.add_argument(\"--disable-gpu\")\n",
    "        \n",
    "        options.add_argument(\"--window-size=720,1080\")\n",
    "        options.add_argument(\"--disable-notifications\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        \n",
    "        # Add user-agent to avoid detection as bot\n",
    "        options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\")\n",
    "        \n",
    "        self.driver = webdriver.Edge(options=options)\n",
    "        return self.driver\n",
    "\n",
    "    def random_delay(self) -> float:\n",
    "        \"\"\"Create random delay if configured.\"\"\"\n",
    "        if self.config[\"random_delay\"]:\n",
    "            delay = random.uniform(self.config[\"min_delay\"], self.config[\"max_delay\"])\n",
    "            time.sleep(delay)\n",
    "            return delay\n",
    "        return 0\n",
    "\n",
    "    def check_and_move_to_next_page(self) -> bool:\n",
    "        \"\"\"Check and move to next page\"\"\"\n",
    "        try:\n",
    "            next_button = self.driver.find_element(By.XPATH, \"//a[text()='Trang sau »']\")\n",
    "            if next_button.is_enabled():\n",
    "                next_button.click()\n",
    "                delay = self.random_delay()\n",
    "                logger.info(f\"Moved to next page (waited {delay:.2f}s)\")\n",
    "                return True\n",
    "            else:\n",
    "                logger.info(\"'Next' button not available or not found.\")\n",
    "                return False\n",
    "        except NoSuchElementException:\n",
    "            logger.info(\"'Next' button not found on this page.\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error moving to next page: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def get_all_urls(self, max_posts: int = 0) -> List[str]:\n",
    "        \"\"\"Get all post URLs from the website, limit if specified.\"\"\"\n",
    "        all_post_url = []\n",
    "        current_page = 1\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                logger.info(f\"Getting URLs from page {current_page}\")\n",
    "                \n",
    "                WebDriverWait(self.driver, 10).until(\n",
    "                    EC.presence_of_all_elements_located((By.XPATH, \"//a[contains(@class,'line-clamp-2')]\"))\n",
    "                )\n",
    "                \n",
    "                post_elements = self.driver.find_elements(By.XPATH, \"//a[contains(@class,'line-clamp-2')]\")\n",
    "                \n",
    "                for element in post_elements:\n",
    "                    url = element.get_attribute('href')\n",
    "                    all_post_url.append(url)\n",
    "                    logger.debug(f\"Added URL: {url}\")\n",
    "                    \n",
    "                    if max_posts > 0 and len(all_post_url) >= max_posts:\n",
    "                        logger.info(f\"Reached limit of {max_posts} posts.\")\n",
    "                        return all_post_url[:max_posts]\n",
    "                \n",
    "                logger.info(f\"Collected {len(all_post_url)} URLs\")\n",
    "                \n",
    "                if not self.check_and_move_to_next_page():\n",
    "                    break\n",
    "                    \n",
    "                current_page += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error getting URLs: {str(e)}\")\n",
    "                break\n",
    "        \n",
    "        return all_post_url\n",
    "\n",
    "    def extract_datetime(self, date_time_str: str) -> str:\n",
    "        \"\"\"Extract date and time from string and format it as 'YYYY-MM-DD HH:MM:SS'.\"\"\"\n",
    "        try:\n",
    "            parts = date_time_str.split(', ')\n",
    "            if len(parts) < 2:\n",
    "                return \"\"\n",
    "\n",
    "            raw_datetime = parts[1] \n",
    "            dt = datetime.strptime(raw_datetime, \"%H:%M %d/%m/%Y\")\n",
    "            return dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting date and time: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    def get_post_content(self) -> str:\n",
    "        \"\"\"Get post content from description.\"\"\"\n",
    "        try:\n",
    "            # Wait for element to load\n",
    "            WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//div[@class='border-bottom pb-3 mb-4']\"))\n",
    "            )\n",
    "            \n",
    "            paragraphs = self.driver.find_elements(By.XPATH, \"//div[@class='border-bottom pb-3 mb-4']/p\")\n",
    "            \n",
    "            post_content_paragraphs = [p.text.strip() for p in paragraphs]\n",
    "            post_content = \"\\n\".join(post_content_paragraphs)\n",
    "            return post_content.strip()  \n",
    "\n",
    "        except TimeoutException:\n",
    "            logger.warning(\"Timeout waiting for description element\")\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting post content: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    def generate_post_id(self, content: str) -> str:\n",
    "        \"\"\"Generate unique ID from post content.\"\"\"\n",
    "        return hashlib.md5(content.encode('utf-8')).hexdigest()\n",
    "\n",
    "    def get_district_and_ward(self, address: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "        \"\"\"Extract district and ward from address string using keyword matching.\"\"\"\n",
    "        if not address or not self.patterns:\n",
    "            return None, None\n",
    "\n",
    "        try:\n",
    "            # Get district list from config\n",
    "            districts = self.patterns.get(\"districts\", [])\n",
    "            wards = self.patterns.get(\"wards\", {})\n",
    "\n",
    "            detected_district = None\n",
    "            for district in districts:\n",
    "                if re.search(r\"\\b\" + re.escape(district) + r\"\\b\", address, re.IGNORECASE):\n",
    "                    detected_district = district\n",
    "                    break\n",
    "\n",
    "            detected_ward = None\n",
    "            if detected_district and detected_district in wards:\n",
    "                for ward in wards[detected_district]:\n",
    "                    if re.search(r\"\\b\" + re.escape(ward) + r\"\\b\", address, re.IGNORECASE):\n",
    "                        detected_ward = ward\n",
    "                        break\n",
    "\n",
    "            # Fall back to simple substring match if regex fails\n",
    "            if not detected_ward and detected_district:\n",
    "                address_lower = address.lower()\n",
    "                for ward in wards[detected_district]:\n",
    "                    if ward.lower() in address_lower:\n",
    "                        detected_ward = ward\n",
    "                        break\n",
    "\n",
    "            return detected_district, detected_ward\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing address '{address}': {str(e)}\")\n",
    "            return None, None\n",
    "        \n",
    "    def get_amenities(self, content: str) -> List[str]:\n",
    "        \"\"\"Get amenities list from post.\"\"\"\n",
    "        if not self.patterns:\n",
    "            return []\n",
    "        # Get amenity patterns from config\n",
    "        amenity_patterns = self.patterns.get(\"amenity_patterns\", {})\n",
    "        detected_amenities = set()\n",
    "        try:\n",
    "            WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((\n",
    "                    By.XPATH,\n",
    "                    \"//div[@class='text-body d-flex pt-1 pb-1' and not(contains(@style, '--bs-text-opacity: 0.1;'))]\")))\n",
    "            amenity_elements = self.driver.find_elements(\n",
    "                By.XPATH,\n",
    "                \"//div[@class='text-body d-flex pt-1 pb-1' and not(contains(@style, '--bs-text-opacity: 0.1;'))]\")\n",
    "            for element in amenity_elements:\n",
    "                text = element.text.strip()\n",
    "                if text:\n",
    "                    matched = False\n",
    "                    for label, pattern in amenity_patterns.items():\n",
    "                        if re.search(pattern, text, re.IGNORECASE):\n",
    "                            detected_amenities.add(label)\n",
    "                            matched = True\n",
    "                            break\n",
    "                    if not matched:\n",
    "                        detected_amenities.add(text) \n",
    "            # Get from content\n",
    "            for label, pattern in amenity_patterns.items():\n",
    "                if re.search(pattern, content, re.IGNORECASE):\n",
    "                    detected_amenities.add(label)\n",
    "            return list(detected_amenities)\n",
    "        except TimeoutException:\n",
    "            logger.warning(\"Timeout waiting for amenity elements\")\n",
    "            return list(detected_amenities)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting amenities: {str(e)}\")\n",
    "            return list(detected_amenities)\n",
    "\n",
    "    def extract_price_value(self, price_str: str) -> Optional[int]:\n",
    "        \"\"\"Extract numeric value from price string and return as integer (VND).\"\"\"\n",
    "        try:\n",
    "            s = price_str.lower().replace('đồng', '').replace('vnd', '').replace('/tháng', '').strip()\n",
    "\n",
    "            if m := re.search(r'(\\d+)[.,](\\d+)\\s*triệu', s):\n",
    "                return int(m.group(1)) * 1_000_000 + int(m.group(2).ljust(2, '0')) * 10_000\n",
    "            elif m := re.search(r'(\\d+)\\s*triệu', s):\n",
    "                return int(m.group(1)) * 1_000_000\n",
    "            elif m := re.search(r'(\\d{3,}(?:[.,]\\d{3})*)', s):\n",
    "                return int(m.group(1).replace('.', '').replace(',', ''))\n",
    "\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing price: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_area_value(self, area_str: str) -> Optional[float | int]:\n",
    "        \"\"\"Extract numeric value from area string.\"\"\"\n",
    "        try:\n",
    "            match = re.search(r'([\\d.,]+)', area_str)\n",
    "            if not match:\n",
    "                return None\n",
    "\n",
    "            number = float(match.group(1).replace(',', '.'))\n",
    "            return number\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing area: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "        \n",
    "    def get_element_text_safely(self, xpath: str, default: str = \"\") -> str:\n",
    "        \"\"\"Safely get text from an element with fallback.\"\"\"\n",
    "        try:\n",
    "            element = self.driver.find_element(By.XPATH, xpath)\n",
    "            return element.text.strip()\n",
    "        except (NoSuchElementException, StaleElementReferenceException):\n",
    "            return default\n",
    "        \n",
    "    def extract_metadata(self) -> Dict[str, Any]:\n",
    "        raw = self.get_element_text_safely(\n",
    "            \"(//td[@class='border-0 pb-0'])[2]\",\n",
    "            self.get_element_text_safely(\"(//table[@class='table table-borderless align-middle m-0'])/tbody//tr[5]\")\n",
    "        )\n",
    "        return {\n",
    "            \"time\": self.extract_datetime(raw)\n",
    "        }\n",
    "\n",
    "    def extract_address_and_location(self) -> Dict[str, Any]:\n",
    "        address = self.get_element_text_safely(\n",
    "            \"(//td[@colspan='3'])[3]\",\n",
    "            self.get_element_text_safely(\"(//table[@class='table table-borderless align-middle m-0'])/tbody//tr[3]/td[2]\")\n",
    "        )\n",
    "        district, ward = self.get_district_and_ward(address)\n",
    "        return {\n",
    "            \"address\": address,\n",
    "            \"district\": district,\n",
    "            \"ward\": ward\n",
    "        }\n",
    "\n",
    "    def extract_info_area(self, content: str) -> Dict[str, Any]:\n",
    "        price_str = self.get_element_text_safely(\n",
    "            \"//span[@class='text-price fs-5 fw-bold']\",\n",
    "            self.get_element_text_safely(\"//span[@class='text-green fs-5 fw-bold']\")\n",
    "        )\n",
    "        area_str = self.get_element_text_safely(\"//div[@class='d-flex justify-content-between']/div/span[3]\")\n",
    "        return {\n",
    "            \"price\": self.extract_price_value(price_str),\n",
    "            \"area\": self.extract_area_value(area_str),\n",
    "            \"amenities\": self.get_amenities(content)\n",
    "        }\n",
    "\n",
    "    def extract_contact(self) -> str:\n",
    "        return self.get_element_text_safely(\n",
    "            \"//div[@class='mb-4']//i[@class='icon telephone-fill white me-2']/..\"\n",
    "        ).strip()\n",
    "\n",
    "    def get_post_data(self, url: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Get post data from URL by extracting parts separately.\"\"\"\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            delay = self.random_delay()\n",
    "            logger.info(f\"Loading page {url} (waited {delay:.2f}s)\")\n",
    "\n",
    "            if \"Page not found\" in self.driver.title or \"Error\" in self.driver.title:\n",
    "                logger.warning(f\"Page doesn't exist or has error: {url}\")\n",
    "                return None\n",
    "\n",
    "            content = self.get_post_content()\n",
    "            post_id = self.generate_post_id(content)\n",
    "\n",
    "            metadata = self.extract_metadata()\n",
    "            address_data = self.extract_address_and_location()\n",
    "            pricing = self.extract_info_area(content)\n",
    "            contact = self.extract_contact()\n",
    "\n",
    "            return {\n",
    "                \"postID\": post_id,\n",
    "                \"time\": metadata[\"time\"],\n",
    "                \"content\": content,\n",
    "                \"address\": address_data[\"address\"],\n",
    "                \"ward\": address_data[\"ward\"],\n",
    "                \"district\": address_data[\"district\"],\n",
    "                \"area\": pricing[\"area\"],\n",
    "                \"price\": pricing[\"price\"],\n",
    "                \"amenities\": pricing[\"amenities\"],\n",
    "                \"contact\": contact,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting data from URL {url}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def save_to_csv(self, data: List[Dict], filename: str) -> bool:\n",
    "        \"\"\"Save data to CSV file.\"\"\"\n",
    "        try:\n",
    "            if not data:\n",
    "                logger.warning(\"No data to save to CSV.\")\n",
    "                return False\n",
    "                \n",
    "            # Get field names from the first item\n",
    "            fieldnames = list(data[0].keys())\n",
    "            \n",
    "            with open(filename, 'w', encoding='utf-8', newline='') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                for row in data:\n",
    "                    row_copy = row.copy()\n",
    "                    if isinstance(row_copy[\"amenities\"], list):\n",
    "                        row_copy[\"amenities\"] = json.dumps(row_copy[\"amenities\"], ensure_ascii=False)\n",
    "                    writer.writerow(row)\n",
    "            \n",
    "            logger.info(f\"Saved data to {filename}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving data to CSV file: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def print_summary(self, post_data_list: List[Dict]):\n",
    "        \"\"\"Print summary of collected data.\"\"\"\n",
    "        if not post_data_list:\n",
    "            print(\"No data collected!\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"🏠 PHONGTRO DATA COLLECTION SUMMARY 🏠\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"✅ Number of posts collected: {len(post_data_list)}\")\n",
    "        \n",
    "        # District stats\n",
    "        districts = {}\n",
    "        for post in post_data_list:\n",
    "            district = post.get(\"district\", \"\")\n",
    "            if district:\n",
    "                districts[district] = districts.get(district, 0) + 1\n",
    "        \n",
    "        if districts:\n",
    "            print(\"\\n📍 Distribution by district:\")\n",
    "            for district, count in sorted(districts.items(), key=lambda x: x[1], reverse=True):\n",
    "                print(f\"  • District {district}: {count} posts\")\n",
    "        \n",
    "        # Price stats\n",
    "        prices = [post.get(\"price\", \"\") for post in post_data_list if post.get(\"price\")]\n",
    "        if prices:\n",
    "            print(\"\\n💰 Price information:\")\n",
    "            print(f\"  • Number of posts with price info: {len(prices)}\")\n",
    "        \n",
    "        print(\"\\n💾 Data saved to: \" + self.config[\"output_file\"])\n",
    "        \n",
    "        if self.config[\"import_to_db\"]:\n",
    "            print(\"📊 Data imported to database\")\n",
    "            \n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    def collect_posts(self, urls: List[str]) -> List[Dict[str, Any]]:\n",
    "        posts = []\n",
    "        for i, url in enumerate(urls):\n",
    "            print(f\"Processing post {i+1}/{len(urls)}\", end='\\r')\n",
    "            logger.info(f\"Processing {i+1}/{len(urls)}: {url}\")\n",
    "            data = self.get_post_data(url)\n",
    "            if data:\n",
    "                posts.append(data)\n",
    "        return posts\n",
    "\n",
    "    def connect_to_db(self):\n",
    "        \"\"\"Connect to the MySQL database.\"\"\"\n",
    "        try:\n",
    "            load_dotenv()\n",
    "            \n",
    "            self.db_connection = mysql.connector.connect(\n",
    "                host=os.getenv('db_host'),\n",
    "                user=os.getenv('db_user'),\n",
    "                password=os.getenv('db_password'),\n",
    "                database=os.getenv('db_name'),\n",
    "                connection_timeout=10\n",
    "            )\n",
    "            self.db_connection.autocommit = False  # Disable autocommit for batch processing\n",
    "            self.db_cursor = self.db_connection.cursor()\n",
    "            logger.info(\"Connected to database\")\n",
    "            return True\n",
    "        except Error as e:\n",
    "            logger.error(f\"Database connection error: {str(e)}\")\n",
    "            print(f\"Database connection error: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def close_db_connection(self):\n",
    "        \"\"\"Close the database connection if it's open.\"\"\"\n",
    "        if self.db_connection and self.db_connection.is_connected():\n",
    "            if self.db_cursor:\n",
    "                self.db_cursor.close()\n",
    "            self.db_connection.close()\n",
    "            logger.info(\"Database connection closed\")\n",
    "            print(\"Database connection closed\")\n",
    "\n",
    "    def import_to_database(self, data: List[Dict[str, Any]]) -> bool:\n",
    "        \"\"\"Import data to MySQL database with upsert (replace if exists).\"\"\"\n",
    "        if not data:\n",
    "            logger.warning(\"No data to import to database\")\n",
    "            return False\n",
    "            \n",
    "        # Connect to database\n",
    "        if not self.connect_to_db():\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            # Check if table exists, if not create it\n",
    "            try:\n",
    "                self.db_cursor.execute(\"\"\"\n",
    "                    CREATE TABLE IF NOT EXISTS post (\n",
    "                        postID VARCHAR(32) PRIMARY KEY,\n",
    "                        p_date DATETIME,\n",
    "                        content LONGTEXT,\n",
    "                        district VARCHAR(255),\n",
    "                        ward VARCHAR(255),\n",
    "                        street_address TEXT,\n",
    "                        price INT,\n",
    "                        area FLOAT,\n",
    "                        amenities JSON,\n",
    "                        contact_info VARCHAR(255),\n",
    "                        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP\n",
    "                    )\n",
    "                \"\"\")\n",
    "                self.db_connection.commit()\n",
    "                logger.info(\"Table 'post' checked/created\")\n",
    "            except Error as e:\n",
    "                logger.error(f\"Error creating table: {str(e)}\")\n",
    "                return False\n",
    "                \n",
    "            # Prepare SQL for inserting or updating\n",
    "            upsert_sql = \"\"\"\n",
    "                INSERT INTO post (\n",
    "                    postID, p_date, content, district, ward,\n",
    "                    street_address, price, area, amenities, contact_info\n",
    "                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                ON DUPLICATE KEY UPDATE\n",
    "                    p_date = VALUES(p_date),\n",
    "                    content = VALUES(content),\n",
    "                    district = VALUES(district),\n",
    "                    ward = VALUES(ward),\n",
    "                    street_address = VALUES(street_address),\n",
    "                    price = VALUES(price),\n",
    "                    area = VALUES(area),\n",
    "                    amenities = VALUES(amenities),\n",
    "                    contact_info = VALUES(contact_info)\n",
    "            \"\"\"\n",
    "            \n",
    "            records_processed = 0\n",
    "            records_updated = 0\n",
    "            records_inserted = 0\n",
    "            \n",
    "            # Process data in batches\n",
    "            for i, row in enumerate(data):\n",
    "                try:\n",
    "                    # Check if post already exists\n",
    "                    check_sql = \"SELECT COUNT(*) FROM post WHERE postID = %s\"\n",
    "                    self.db_cursor.execute(check_sql, (row[\"postID\"],))\n",
    "                    exists = self.db_cursor.fetchone()[0] > 0\n",
    "                    \n",
    "                    amenities_json = json.dumps(row[\"amenities\"], ensure_ascii=False) if isinstance(row[\"amenities\"], list) else row[\"amenities\"]\n",
    "                    \n",
    "                    # Prepare values tuple\n",
    "                    values = (\n",
    "                        row[\"postID\"],\n",
    "                        row[\"time\"],\n",
    "                        row[\"content\"],\n",
    "                        row[\"district\"],\n",
    "                        row[\"ward\"],\n",
    "                        row[\"address\"],\n",
    "                        row[\"price\"],\n",
    "                        row[\"area\"],\n",
    "                        amenities_json,\n",
    "                        row[\"contact\"]\n",
    "                    )\n",
    "                    \n",
    "                    # Try to insert/update with retries\n",
    "                    for attempt in range(self.config[\"db_retry_limit\"]):\n",
    "                        try:\n",
    "                            self.db_cursor.execute(upsert_sql, values)\n",
    "                            records_processed += 1\n",
    "                            \n",
    "                            if exists:\n",
    "                                records_updated += 1\n",
    "                            else:\n",
    "                                records_inserted += 1\n",
    "                                \n",
    "                            break \n",
    "                        except mysql.connector.errors.DatabaseError as e:\n",
    "                            if \"Lock wait timeout exceeded\" in str(e) and attempt < self.config[\"db_retry_limit\"] - 1:\n",
    "                                logger.warning(f\"Lock timeout on row {i}, retrying ({attempt + 1}/{self.config['db_retry_limit']})...\")\n",
    "                                time.sleep(2)\n",
    "                            else:\n",
    "                                raise\n",
    "                    \n",
    "                    # Commit every batch_size records\n",
    "                    if i % self.config[\"db_batch_size\"] == 0 and i > 0:\n",
    "                        self.db_connection.commit()\n",
    "                        logger.info(f\"Committed batch of {self.config['db_batch_size']} records (total: {records_processed})\")\n",
    "                        print(f\"Processed {records_processed} records ({records_inserted} new, {records_updated} updated)\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing row {i}: {str(e)}\")\n",
    "                    print(f\"Error processing row {i}: {str(e)}\")\n",
    "            \n",
    "            # Final commit for remaining records\n",
    "            self.db_connection.commit()\n",
    "            logger.info(f\"Database import complete. Total: {records_processed} records ({records_inserted} new, {records_updated} updated)\")\n",
    "            print(f\"Database import complete. Total: {records_processed} records ({records_inserted} new, {records_updated} updated)\")\n",
    "            return True\n",
    "            \n",
    "        except Error as e:\n",
    "            logger.error(f\"Database error: {str(e)}\")\n",
    "            print(f\"Database error: {str(e)}\")\n",
    "            return False\n",
    "            \n",
    "        finally:\n",
    "            self.close_db_connection()\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run the complete workflow: scrape data, save to CSV, and import to database.\"\"\"\n",
    "        self.print_header()\n",
    "        if not self.patterns:\n",
    "            print(\"Error: Could not load config.json.\")\n",
    "            return\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.setup_driver()\n",
    "\n",
    "        try:\n",
    "            self.driver.get(f\"https://phongtro123.com/tinh-thanh/{self.config['city']}?orderby=moi-nhat\")\n",
    "            self.random_delay()\n",
    "\n",
    "            urls = self.get_all_urls(self.config[\"post_limit\"])\n",
    "            posts = self.collect_posts(urls)\n",
    "\n",
    "            if posts:\n",
    "                # Save to CSV\n",
    "                self.save_to_csv(posts, self.config[\"output_file\"])\n",
    "                \n",
    "                # Import to database if configured\n",
    "                if self.config[\"import_to_db\"]:\n",
    "                    self.import_to_database(posts)\n",
    "                    \n",
    "                self.print_summary(posts)\n",
    "            else:\n",
    "                print(\"No data collected.\")\n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "            print(f\"⏱️ Execution time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    \n",
    "    def print_header(self):\n",
    "        \"\"\"Print program header.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"🏠 PHONGTRO DATA SCRAPER & DATABASE IMPORTER 🏠\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"• City: {self.config['city']}\")\n",
    "        print(f\"• Post limit: {self.config['post_limit'] if self.config['post_limit'] > 0 else 'No limit'}\")\n",
    "        print(f\"• Output file: {self.config['output_file']}\")\n",
    "        print(f\"• Headless mode: {'On' if self.config['headless'] else 'Off'}\")\n",
    "        print(f\"• Import to database: {'Yes' if self.config['import_to_db'] else 'No'}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = DateScraper(DEFAULT_CONFIG)\n",
    "    scraper.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
